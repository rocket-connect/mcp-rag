import { writeFileSync, mkdirSync } from 'fs'
import { join } from 'path'

export interface BenchmarkResult {
  timestamp: string
  commit?: string
  branch?: string
  summary?: BenchmarkSummary
}

export interface BenchmarkSummary {
  totalTests: number
  successfulTests: number
  totalResponseTime: number
  averageResponseTime: number
  totalTokens: number
  averageTokens: number
  toolCallSuccessRate: number
  metrics: RequestMetric[]
}

export interface RequestMetric {
  promptNumber: number
  prompt: string
  toolCalled: string | null
  tokenCount: number
  responseTime: number
  conversationLength: number
}

export function generateMarkdownReport(result: BenchmarkResult): string {
  const sections: string[] = []

  // Header
  sections.push('# MCP-RAG Benchmark Results')
  sections.push('')
  sections.push(`**Generated**: ${new Date(result.timestamp).toLocaleString()}`)

  if (result.commit) {
    sections.push(`**Commit**: \`${result.commit}\``)
  }

  if (result.branch) {
    sections.push(`**Branch**: \`${result.branch}\``)
  }

  sections.push('')

  // Summary section
  if (result.summary) {
    sections.push('## üìä Summary')
    sections.push('')
    sections.push('| Metric | Value |')
    sections.push('|--------|-------|')
    sections.push(`| **Total Tests** | ${result.summary.totalTests} |`)
    sections.push(
      `| **Successful Tool Calls** | ${result.summary.successfulTests}/${result.summary.totalTests} |`
    )
    sections.push(
      `| **Success Rate** | ${result.summary.toolCallSuccessRate.toFixed(1)}% |`
    )
    sections.push(
      `| **Total Response Time** | ${result.summary.totalResponseTime}ms |`
    )
    sections.push(
      `| **Avg Response Time** | ${result.summary.averageResponseTime}ms |`
    )
    sections.push(`| **Total Tokens** | ${result.summary.totalTokens} |`)
    sections.push(`| **Avg Tokens** | ${result.summary.averageTokens} |`)
    sections.push('')

    // Performance metrics
    const responseTimes = result.summary.metrics.map(m => m.responseTime)
    const tokenCounts = result.summary.metrics.map(m => m.tokenCount)

    sections.push('### Performance Breakdown')
    sections.push('')
    sections.push('**Response Times:**')
    sections.push(`- Min: ${Math.min(...responseTimes)}ms`)
    sections.push(`- Max: ${Math.max(...responseTimes)}ms`)
    sections.push(`- Avg: ${result.summary.averageResponseTime}ms`)
    sections.push('')
    sections.push('**Token Usage:**')
    sections.push(`- Min: ${Math.min(...tokenCounts)} tokens`)
    sections.push(`- Max: ${Math.max(...tokenCounts)} tokens`)
    sections.push(`- Avg: ${result.summary.averageTokens} tokens`)
    sections.push('')

    // Detailed metrics table
    sections.push('## üìã Detailed Results')
    sections.push('')
    sections.push(
      '| # | Tool Called | Tokens | Response Time | Messages | Prompt |'
    )
    sections.push(
      '|---|-------------|--------|---------------|----------|--------|'
    )

    result.summary.metrics.forEach(m => {
      const toolName = m.toolCalled || 'None'
      const promptPreview =
        m.prompt.length > 50 ? m.prompt.substring(0, 47) + '...' : m.prompt

      sections.push(
        `| ${m.promptNumber} | ${toolName} | ${m.tokenCount} | ${m.responseTime}ms | ${m.conversationLength} | ${promptPreview} |`
      )
    })
    sections.push('')

    // Tool usage breakdown
    const toolUsage = result.summary.metrics.reduce(
      (acc, m) => {
        if (m.toolCalled) {
          acc[m.toolCalled] = (acc[m.toolCalled] || 0) + 1
        }
        return acc
      },
      {} as Record<string, number>
    )

    if (Object.keys(toolUsage).length > 0) {
      sections.push('## üîß Tool Usage')
      sections.push('')
      sections.push('| Tool | Count |')
      sections.push('|------|-------|')

      Object.entries(toolUsage)
        .sort((a, b) => b[1] - a[1])
        .forEach(([tool, count]) => {
          sections.push(`| ${tool} | ${count} |`)
        })
      sections.push('')
    }
  } else {
    sections.push('## ‚ÑπÔ∏è No benchmark data available')
    sections.push('')
    sections.push(
      'Run benchmarks with metrics collection enabled to see detailed results.'
    )
    sections.push('')
  }

  // Footer
  sections.push('---')
  sections.push('')
  sections.push('*Generated by MCP-RAG Benchmark Suite*')

  return sections.join('\n')
}

export function saveReport(content: string, filename: string): void {
  const resultsDir = join(process.cwd(), 'results')
  mkdirSync(resultsDir, { recursive: true })

  const filepath = join(resultsDir, filename)
  writeFileSync(filepath, content)
  console.log(`‚úÖ Report saved to ${filepath}`)
}

export function saveHistoricalReport(content: string): void {
  const timestamp = new Date()
    .toISOString()
    .replace(/[:.]/g, '-')
    .split('T')
    .join('-')
    .slice(0, -5)
  const filename = `${timestamp}.md`
  const historyPath = join(process.cwd(), 'results', 'history', filename)
  const historyDir = join(process.cwd(), 'results', 'history')

  mkdirSync(historyDir, { recursive: true })
  writeFileSync(historyPath, content)
  console.log(`üìä Historical report saved to ${historyPath}`)
}
